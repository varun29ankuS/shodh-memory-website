import { Header } from "@/components/Header";
import { Footer } from "@/components/Footer";
import Link from "next/link";
import { notFound } from "next/navigation";
import { BLOG_POSTS } from "../page";

const BLOG_CONTENT: Record<string, string[]> = {
  "memory-architecture-autonomous-agents": [
    "# Memory Architecture for Autonomous Agents",
    "",
    "Autonomous agents are having a moment. Coding assistants that understand your codebase. Research agents that synthesize papers. Robotic systems that adapt to warehouses. The agentic future is here.",
    "",
    "There's just one problem: most agents are goldfish.",
    "",
    "## The Goldfish Problem",
    "",
    "Ask Claude to help you debug. Great answers. Come back tomorrow with a related question. No memory of yesterday's context. Every session starts from zero.",
    "",
    "This isn't a limitation of the underlying models—it's an architecture failure. We give agents massive brains (GPT-4, Claude, Gemini) but no persistent memory. It's like having a genius consultant with amnesia.",
    "",
    "## What Autonomous Agents Actually Need",
    "",
    "Real autonomy requires memory that:",
    "",
    "1. **Persists across sessions** — Yesterday's context should inform today's work",
    "2. **Learns what matters** — Frequently-used knowledge should strengthen",
    "3. **Forgets what doesn't** — Noise should decay naturally",
    "4. **Connects related concepts** — Accessing one memory should prime related ones",
    "5. **Works offline** — Edge agents can't phone home for every decision",
    "",
    "Vector databases give you (1). Sort of. But they miss 2-5 entirely. That's not memory. That's storage.",
    "",
    "## The Architecture: Past, Present, and Future",
    "",
    "Here's the key insight that changes everything:",
    "",
    "**Both past AND future should inform the present.**",
    "",
    "When you ask an agent a question, it should consider:",
    "",
    "- **Past context**: What have we discussed before? What decisions were made?",
    "- **Present query**: What are you asking right now?",
    "- **Future intentions**: What are you trying to accomplish? What todos are pending?",
    "",
    "Most systems only consider present + maybe recent past. That's tunnel vision.",
    "",
    "### A Concrete Example",
    "",
    "You're building a web app. Three weeks ago, you decided to use PostgreSQL. Last week, you added a todo: \"optimize database queries.\" Today, you ask: \"How should I structure this data?\"",
    "",
    "A goldfish agent: Suggests whatever. Maybe MongoDB. Who knows.",
    "",
    "A brain-equipped agent:",
    "- Recalls: \"This project uses PostgreSQL\" (past decision)",
    "- Sees: \"optimize database queries\" (future intention)",
    "- Responds: \"Given your PostgreSQL setup and upcoming optimization work, here's a normalized schema that indexes well...\"",
    "",
    "The difference is staggering.",
    "",
    "## The Three-Tier Model",
    "",
    "Cognitive science tells us memory isn't monolithic. Nelson Cowan's embedded-processes model describes three tiers:",
    "",
    "```",
    "┌─────────────────────────────────────────────────┐",
    "│  SENSORY BUFFER                                 │",
    "│  Immediate input, ~7 items, decays in seconds   │",
    "└─────────────────────┬───────────────────────────┘",
    "                      │ attention",
    "                      ▼",
    "┌─────────────────────────────────────────────────┐",
    "│  WORKING MEMORY                                 │",
    "│  Active context, ~4 chunks, decays in minutes   │",
    "└─────────────────────┬───────────────────────────┘",
    "                      │ consolidation",
    "                      ▼",
    "┌─────────────────────────────────────────────────┐",
    "│  LONG-TERM MEMORY                               │",
    "│  Persistent storage, unlimited, power-law decay │",
    "└─────────────────────────────────────────────────┘",
    "```",
    "",
    "Information flows through tiers. Important things consolidate. Noise fades.",
    "",
    "## Hebbian Learning: Connections That Strengthen",
    "",
    "Donald Hebb's 1949 principle: \"Neurons that fire together wire together.\"",
    "",
    "When two memories are accessed together, their connection should strengthen. This creates associative networks—think of one concept, and related concepts automatically activate.",
    "",
    "```python",
    "# Pseudocode for Hebbian strengthening",
    "def on_co_access(memory_a, memory_b):",
    "    edge = graph.get_edge(memory_a, memory_b)",
    "    if edge:",
    "        edge.strength += LEARNING_RATE * (1 - edge.strength)",
    "    else:",
    "        graph.create_edge(memory_a, memory_b, initial_strength=0.1)",
    "```",
    "",
    "Over time, core knowledge (user preferences, key decisions) becomes strongly connected. Ephemeral context stays weakly linked and eventually fades.",
    "",
    "## Decay: The Feature, Not the Bug",
    "",
    "Most engineers think forgetting is a failure. It's actually essential.",
    "",
    "Without decay:",
    "- Memory fills with noise",
    "- Retrieval quality collapses",
    "- Old, irrelevant context competes with current needs",
    "",
    "With intelligent decay:",
    "- Unused memories fade naturally",
    "- Frequently-accessed knowledge persists",
    "- The signal-to-noise ratio improves over time",
    "",
    "The math matters. Ebbinghaus showed forgetting follows predictable curves. We use hybrid exponential + power-law decay based on Wixted's research. Recent memories decay fast (exponential). Older memories have a long tail (power-law).",
    "",
    "## Prospective Memory: The Future Informs Present",
    "",
    "Here's what most systems miss entirely: **intentions**.",
    "",
    "When you create a todo, that's a future intention. It should influence what context surfaces NOW.",
    "",
    "```python",
    "# Prospective memory integration",
    "def get_context(query):",
    "    # Standard: semantic search on past memories",
    "    past = vector_search(query)",
    "    ",
    "    # Novel: pending intentions that relate to query",
    "    future = search_todos_and_reminders(query)",
    "    ",
    "    # Combine for full temporal context",
    "    return fuse_past_and_future(past, future)",
    "```",
    "",
    "This is what makes an agent feel like it \"gets\" you. It's not just remembering what you said. It's understanding where you're going.",
    "",
    "## The Practical Stack",
    "",
    "A real implementation needs:",
    "",
    "| Component | Purpose |",
    "|-----------|---------|",
    "| Vector Index | Semantic similarity search |",
    "| Knowledge Graph | Entity relationships + Hebbian learning |",
    "| Temporal Index | Time-based retrieval + decay |",
    "| Todo/Intention Store | Prospective memory |",
    "| Consolidation Loop | Background maintenance + strengthening |",
    "",
    "All of this needs to run fast (<50ms for context retrieval) and work offline (no cloud dependency for every decision).",
    "",
    "## Results: What Changes",
    "",
    "When you give an agent real memory architecture:",
    "",
    "| Metric | Goldfish Agent | Brain-Equipped Agent |",
    "|--------|----------------|----------------------|",
    "| Cross-session context | None | Full |",
    "| Retrieval precision | ~67% | ~86% |",
    "| Response relevance | Generic | Personalized |",
    "| Knowledge decay | Everything persists | Noise fades |",
    "| Offline capability | No | Yes |",
    "",
    "The numbers matter less than the experience. An agent with memory feels like a colleague. An agent without feels like a search engine.",
    "",
    "## Getting Started",
    "",
    "If you're building autonomous agents, stop treating memory as an afterthought. The architecture choices you make now determine whether your agent is a goldfish or a brain.",
    "",
    "Key decisions:",
    "",
    "1. **Don't just use a vector database** — Add knowledge graphs for relationships",
    "2. **Implement decay** — Your future self will thank you when the database isn't full of noise",
    "3. **Consider prospective memory** — Todos and intentions should inform context",
    "4. **Plan for offline** — Edge deployment is often necessary",
    "",
    "We've open-sourced our implementation at [shodh-memory](https://github.com/varun29ankuS/shodh-memory). It's Rust-based, runs offline, and implements everything described here. Single binary, no cloud required.",
    "",
    "The agentic future needs agents that remember. Time to build brains, not databases.",
  ],
  "hebbian-learning-ai-agents": [
    "# Hebbian Learning for AI Agents",
    "",
    "Donald Hebb's 1949 insight changed neuroscience forever: \"Neurons that fire together wire together.\" When two neurons repeatedly activate in sequence, their synaptic connection strengthens. This simple principle underlies how biological brains learn.",
    "",
    "## The Problem with Static Memory",
    "",
    "Most AI memory systems treat all memories equally. Store a fact, retrieve it later, done. But this misses a crucial insight: not all memories are equally important.",
    "",
    "Consider an AI coding assistant. It might remember:",
    "- The user prefers Rust over Python (accessed 50 times)",
    "- A one-off debugging session from 3 months ago (accessed once)",
    "",
    "Static systems give these equal weight. Hebbian learning doesn't.",
    "",
    "## How We Implement Hebbian Learning",
    "",
    "In shodh-memory, every knowledge graph edge has a `strength` value (0.0 to 1.0). When two memories are accessed together:",
    "",
    "```rust",
    "fn strengthen_connection(edge: &mut Edge, activation: f32) {",
    "    let delta = LEARNING_RATE * activation * (1.0 - edge.strength);",
    "    edge.strength = (edge.strength + delta).min(1.0);",
    "}",
    "```",
    "",
    "The key insight: strengthening is proportional to remaining capacity. Weak connections strengthen quickly; strong connections plateau. This matches biological long-term potentiation (LTP).",
    "",
    "## Decay Without Use",
    "",
    "Hebbian learning has a corollary: connections that don't fire weaken. We implement this with hybrid decay:",
    "",
    "```rust",
    "fn apply_decay(edge: &mut Edge, hours_since_access: f32) {",
    "    // Exponential decay for recent memories",
    "    let exp_decay = (-hours_since_access / HALF_LIFE).exp();",
    "    // Power-law for older memories (slower decay)",
    "    let power_decay = (1.0 + hours_since_access).powf(-0.5);",
    "    // Blend based on memory age",
    "    edge.strength *= exp_decay * 0.7 + power_decay * 0.3;",
    "}",
    "```",
    "",
    "This matches Ebbinghaus's forgetting curve research: rapid initial decay, then a long tail.",
    "",
    "## Long-Term Potentiation (LTP)",
    "",
    "In biology, some synapses become permanent through repeated activation. We implement LTP by marking edges as `permanent` once they cross a threshold:",
    "",
    "```rust",
    "if edge.strength > LTP_THRESHOLD && edge.access_count > LTP_MIN_ACCESSES {",
    "    edge.permanent = true;  // Immune to decay",
    "}",
    "```",
    "",
    "This means core knowledge (the user prefers Rust) becomes permanent, while ephemeral context fades naturally.",
    "",
    "## Results",
    "",
    "In our testing, Hebbian learning improved relevant context retrieval by 34% compared to static memory. More importantly, it reduced noise—old, irrelevant memories naturally fade instead of cluttering results.",
    "",
    "Memory should work like memory. Shodh-memory makes it so.",
  ],
  "edge-ai-memory-raspberry-pi": [
    "# Running AI Memory on a Raspberry Pi",
    "",
    "Edge AI is only useful if it actually runs on edge devices. Here's how to deploy shodh-memory on a Raspberry Pi 4/5 and achieve sub-100ms semantic search.",
    "",
    "## Why Raspberry Pi?",
    "",
    "The Pi represents the baseline for edge computing:",
    "- Cheap ($35-75)",
    "- Low power (5-15W)",
    "- Limited RAM (1-8GB)",
    "- ARM architecture",
    "",
    "If your AI memory system can't run here, it's not really edge-ready.",
    "",
    "## Installation",
    "",
    "```bash",
    "# On Raspberry Pi OS (64-bit recommended)",
    "curl -L https://github.com/varun29ankuS/shodh-memory/releases/download/v0.1.6/shodh-memory-aarch64-linux -o shodh-memory",
    "chmod +x shodh-memory",
    "./shodh-memory --data-dir ./memory-data",
    "```",
    "",
    "That's it. Single binary, no Python, no npm, no Docker.",
    "",
    "## Memory Configuration",
    "",
    "For a Pi 4 with 4GB RAM, we recommend:",
    "",
    "```toml",
    "# config.toml",
    "[memory]",
    "max_memories = 50000",
    "embedding_cache_size = 1000",
    "graph_cache_size = 10000",
    "",
    "[performance]",
    "worker_threads = 4",
    "batch_size = 32",
    "```",
    "",
    "## Benchmark Results",
    "",
    "On a Raspberry Pi 4 (4GB, arm64):",
    "",
    "| Operation | Latency (p50) | Latency (p99) |",
    "|-----------|---------------|---------------|",
    "| Graph lookup | 0.8μs | 2.1μs |",
    "| Remember | 45ms | 89ms |",
    "| Recall (semantic) | 67ms | 142ms |",
    "| Proactive context | 23ms | 51ms |",
    "",
    "These numbers assume warm cache. Cold start adds ~200ms for model loading.",
    "",
    "## Integration with Robotics",
    "",
    "For ROS2 integration:",
    "",
    "```python",
    "from shodh_memory import Memory",
    "import rclpy",
    "",
    "class MemoryNode(Node):",
    "    def __init__(self):",
    "        super().__init__('memory_node')",
    "        self.memory = Memory('./robot_memory')",
    "        self.create_subscription(",
    "            String, 'observations', self.observe, 10)",
    "",
    "    def observe(self, msg):",
    "        self.memory.remember(msg.data, tags=['observation'])",
    "```",
    "",
    "## Power Consumption",
    "",
    "- Idle: 2.1W",
    "- Active inference: 4.3W",
    "- Peak (embedding): 5.8W",
    "",
    "A 10,000mAh battery provides ~6 hours of active use.",
    "",
    "## Conclusion",
    "",
    "Edge AI memory isn't a future promise—it's available now. The Pi proves that meaningful AI can run on meaningful hardware constraints.",
  ],
  "three-tier-memory-architecture": [
    "# The Three-Tier Memory Architecture",
    "",
    "Human memory isn't a single system—it's layers of systems with different capacities, decay rates, and access patterns. Shodh-memory implements this insight.",
    "",
    "## Cowan's Embedded-Processes Model",
    "",
    "Psychologist Nelson Cowan proposed that working memory isn't separate from long-term memory—it's an activated subset of it. His model has three components:",
    "",
    "1. **Sensory Buffer**: Raw input, ~7 items, decays in seconds",
    "2. **Focus of Attention**: Active processing, ~4 chunks, decays in minutes",
    "3. **Activated Long-Term Memory**: Primed memories, unlimited, decays over hours/days",
    "",
    "Traditional AI memory systems ignore this. They dump everything into a vector store and hope for the best.",
    "",
    "## Our Implementation",
    "",
    "### Tier 1: Sensory Buffer",
    "",
    "```rust",
    "pub struct SensoryBuffer {",
    "    items: RingBuffer<RawInput, 7>,",
    "    ttl: Duration::from_secs(30),",
    "}",
    "```",
    "",
    "Raw observations enter here first. Most are discarded. Only salient inputs (determined by novelty detection) graduate to working memory.",
    "",
    "### Tier 2: Working Memory",
    "",
    "```rust",
    "pub struct WorkingMemory {",
    "    focus: BoundedVec<Chunk, 4>,",
    "    associations: HashMap<ChunkId, Vec<LtmId>>,",
    "    decay_rate: f32,  // Minutes",
    "}",
    "```",
    "",
    "Working memory maintains the current context. When you ask \"What was I working on?\", this is what answers. It holds ~4 chunks but each chunk can reference many long-term memories.",
    "",
    "### Tier 3: Long-Term Memory",
    "",
    "```rust",
    "pub struct LongTermMemory {",
    "    episodic: VectorIndex,      // What happened",
    "    semantic: KnowledgeGraph,   // What it means",
    "    decay: HybridDecay,         // Exponential + power-law",
    "}",
    "```",
    "",
    "Long-term memory is where meaning lives. It combines episodic memories (events) with semantic knowledge (relationships).",
    "",
    "## Information Flow",
    "",
    "```",
    "Input → Sensory Buffer → (filter) → Working Memory → (consolidate) → LTM",
    "                                          ↑                          ↓",
    "                                          └──── (retrieve) ──────────┘",
    "```",
    "",
    "The key insight: retrieval from LTM into working memory is where Hebbian learning happens. Accessed memories strengthen; ignored memories decay.",
    "",
    "## Why This Matters",
    "",
    "Single-tier memory systems have no notion of relevance. Everything is equally accessible, which means everything competes for attention. Our tiered approach means:",
    "",
    "- Recent context is always fast (working memory)",
    "- Important patterns persist (LTM with strengthening)",
    "- Noise fades naturally (decay)",
    "",
    "This matches human memory because it's modeled on human memory.",
  ],
  "why-not-just-vector-search": [
    "# Why Vector Search Alone Isn't Enough",
    "",
    "Vector databases are having a moment. But for AI agent memory, they're necessary—not sufficient.",
    "",
    "## What Vectors Do Well",
    "",
    "Semantic similarity. Given a query, find memories that mean similar things. This is genuinely useful:",
    "",
    "```",
    "Query: \"How do I optimize database queries?\"",
    "Match: \"Use EXPLAIN ANALYZE to find slow operations\"",
    "```",
    "",
    "The query and match share no keywords but are semantically related. Vectors handle this beautifully.",
    "",
    "## What Vectors Miss",
    "",
    "### 1. Relationships",
    "",
    "Vectors represent points in space. But memory is a graph. Consider:",
    "",
    "- \"The user prefers Rust\" (node A)",
    "- \"The user is building a web server\" (node B)",
    "- A → B implies: prefer Axum over Express",
    "",
    "Vector search finds A and B independently. It doesn't understand that A should influence recommendations when B is active.",
    "",
    "### 2. Temporal Context",
    "",
    "Vectors are timeless. But memory has sequence:",
    "",
    "- 9:00am: User starts debugging auth flow",
    "- 9:15am: User asks about JWT tokens",
    "- 9:30am: User asks about token refresh",
    "",
    "A pure vector search for \"token\" might surface unrelated token memories from weeks ago. Temporal context knows that recent auth-related memories are more relevant.",
    "",
    "### 3. Importance",
    "",
    "All vectors are born equal. But not all memories matter equally:",
    "",
    "- \"User mentioned liking dark mode\" (strength: 0.3)",
    "- \"User's production system uses PostgreSQL\" (strength: 0.9)",
    "",
    "Vector similarity doesn't capture that the PostgreSQL fact is load-bearing knowledge while dark mode is a preference.",
    "",
    "## Our Hybrid Approach",
    "",
    "Shodh-memory combines three systems:",
    "",
    "```rust",
    "pub struct MemoryCore {",
    "    vectors: HnswIndex,          // Semantic similarity",
    "    graph: KnowledgeGraph,       // Relationships",
    "    temporal: TemporalIndex,     // Time-based access",
    "}",
    "```",
    "",
    "Retrieval fuses all three signals:",
    "",
    "```rust",
    "fn retrieve(query: &str) -> Vec<Memory> {",
    "    let semantic = self.vectors.search(query, 20);",
    "    let graph = self.graph.spread_activation(query);",
    "    let temporal = self.temporal.recent_context();",
    "",
    "    // RRF fusion with learned weights",
    "    fuse_results(semantic, graph, temporal)",
    "}",
    "```",
    "",
    "## Benchmarks",
    "",
    "On our agent-task benchmark (coding assistant scenarios):",
    "",
    "| Approach | Precision@5 | MRR |",
    "|----------|-------------|-----|",
    "| Vector only | 0.67 | 0.71 |",
    "| Vector + Graph | 0.79 | 0.83 |",
    "| Full hybrid | 0.86 | 0.89 |",
    "",
    "The graph adds relationship context. Temporal adds recency bias. Together they significantly outperform vectors alone.",
  ],
  "memory-decay-forgetting-curves": [
    "# Memory Decay and Forgetting Curves",
    "",
    "Hermann Ebbinghaus ran memory experiments on himself in the 1880s. His findings still guide how we build AI memory systems.",
    "",
    "## The Ebbinghaus Forgetting Curve",
    "",
    "Ebbinghaus memorized nonsense syllables and tested recall over time. His results:",
    "",
    "- After 20 minutes: 58% retention",
    "- After 1 hour: 44% retention",
    "- After 1 day: 34% retention",
    "- After 1 week: 25% retention",
    "- After 1 month: 21% retention",
    "",
    "The pattern: rapid initial decay, then a long tail.",
    "",
    "## Exponential vs Power-Law Decay",
    "",
    "Two mathematical models fit forgetting:",
    "",
    "**Exponential decay**: R(t) = e^(-t/τ)",
    "- Fast initial drop",
    "- Good for recent memories",
    "",
    "**Power-law decay**: R(t) = (1 + t)^(-β)",
    "- Slower, more gradual",
    "- Good for older memories",
    "",
    "Research shows human memory uses BOTH. Recent memories decay exponentially; older memories follow power-law.",
    "",
    "## Our Hybrid Implementation",
    "",
    "```rust",
    "fn calculate_retention(hours: f32, initial_strength: f32) -> f32 {",
    "    // Exponential component (half-life of 24 hours)",
    "    let exp_decay = (-hours / 24.0).exp();",
    "",
    "    // Power-law component (β = 0.5)",
    "    let power_decay = (1.0 + hours).powf(-0.5);",
    "",
    "    // Blend: more exponential early, more power-law later",
    "    let blend = (hours / 168.0).min(1.0);  // 1 week transition",
    "    let retention = exp_decay * (1.0 - blend) + power_decay * blend;",
    "",
    "    initial_strength * retention",
    "}",
    "```",
    "",
    "## Spacing Effect",
    "",
    "Ebbinghaus also discovered the spacing effect: memories reviewed at increasing intervals are retained longer. We implement this:",
    "",
    "```rust",
    "fn on_access(memory: &mut Memory) {",
    "    let interval = memory.last_access - memory.previous_access;",
    "    if interval > memory.optimal_interval {",
    "        // Spaced retrieval strengthens more",
    "        memory.strength += BOOST * 1.5;",
    "        memory.optimal_interval *= 2.0;  // Increase next interval",
    "    } else {",
    "        memory.strength += BOOST;",
    "    }",
    "}",
    "```",
    "",
    "## Consolidation During Idle",
    "",
    "The brain consolidates memories during sleep. Shodh-memory runs a consolidation loop during idle periods:",
    "",
    "```rust",
    "async fn consolidation_loop() {",
    "    loop {",
    "        sleep(Duration::from_secs(300)).await;",
    "        apply_decay_to_all_memories();",
    "        replay_important_memories();  // Like dreaming",
    "        prune_weak_connections();",
    "    }",
    "}",
    "```",
    "",
    "This ensures memory quality degrades gracefully without active maintenance.",
    "",
    "## Results",
    "",
    "With hybrid decay, our memory quality over time:",
    "",
    "- Day 1: 94% relevant context",
    "- Week 1: 87% relevant context",
    "- Month 1: 71% relevant context",
    "",
    "Without decay, noise accumulates until retrieval quality collapses.",
  ],
  "mcp-integration-claude-cursor": [
    "# Integrating shodh-memory with Claude Code and Cursor",
    "",
    "The Model Context Protocol (MCP) lets AI assistants use external tools. Here's how to give Claude Code persistent memory in 60 seconds.",
    "",
    "## Quick Start",
    "",
    "```bash",
    "# One command to add memory to Claude Code",
    "claude mcp add shodh-memory npx -y @shodh/memory-mcp",
    "```",
    "",
    "That's it. Claude now remembers across sessions.",
    "",
    "## What Claude Can Now Do",
    "",
    "After installation, Claude has these new capabilities:",
    "",
    "### remember",
    "Store important context for later:",
    "```",
    "User: Remember that this project uses PostgreSQL 15",
    "Claude: [Stores memory with tags: database, postgresql, infrastructure]",
    "```",
    "",
    "### recall",
    "Search past context semantically:",
    "```",
    "User: What database does this project use?",
    "Claude: [Recalls: \"This project uses PostgreSQL 15\"]",
    "```",
    "",
    "### proactive_context",
    "Automatically surface relevant memories:",
    "```",
    "User: Help me optimize this SQL query",
    "Claude: [Auto-retrieves database preferences, past optimization discussions]",
    "```",
    "",
    "## Cursor Integration",
    "",
    "For Cursor, add to your MCP settings:",
    "",
    "```json",
    "{",
    "  \"mcpServers\": {",
    "    \"shodh-memory\": {",
    "      \"command\": \"npx\",",
    "      \"args\": [\"-y\", \"@shodh/memory-mcp\"],",
    "      \"env\": {",
    "        \"SHODH_USER_ID\": \"cursor-user\"",
    "      }",
    "    }",
    "  }",
    "}",
    "```",
    "",
    "## Memory Isolation",
    "",
    "Each user/project can have isolated memory:",
    "",
    "```bash",
    "# Per-project memory",
    "SHODH_USER_ID=project-frontend npx @shodh/memory-mcp",
    "",
    "# Per-user memory",
    "SHODH_USER_ID=$(whoami) npx @shodh/memory-mcp",
    "```",
    "",
    "## What Gets Remembered",
    "",
    "By default, shodh-memory stores:",
    "- Explicit user preferences",
    "- Important decisions and their rationale",
    "- Learned patterns about the codebase",
    "- Error resolutions for future reference",
    "",
    "It does NOT store:",
    "- Conversation logs (privacy)",
    "- Secrets or credentials (security)",
    "- Temporary debugging context",
    "",
    "## Verification",
    "",
    "Test that memory is working:",
    "",
    "```",
    "User: Remember that I prefer functional programming patterns",
    "Claude: Stored. I'll keep this preference in mind.",
    "",
    "[New session]",
    "",
    "User: How should I structure this data transformation?",
    "Claude: Based on your preference for functional patterns,",
    "        I'd suggest using map/filter/reduce rather than loops...",
    "```",
    "",
    "## Troubleshooting",
    "",
    "```bash",
    "# Check if MCP server is running",
    "claude mcp list",
    "",
    "# View stored memories",
    "curl http://localhost:3030/api/memories",
    "",
    "# Reset memory (fresh start)",
    "rm -rf ~/.shodh-memory/data",
    "```",
  ],
  "knowledge-graph-spreading-activation": [
    "# Knowledge Graphs and Spreading Activation",
    "",
    "Vector search finds similar documents. Knowledge graphs find connected concepts. Together, they enable proactive context.",
    "",
    "## The Spreading Activation Model",
    "",
    "In cognitive psychology, spreading activation explains how thinking of one concept primes related concepts:",
    "",
    "```",
    "Think: \"dog\"",
    "Activates: pet → animal → bark → leash → walk → park",
    "```",
    "",
    "Activation spreads along associative links, weakening with distance. This is how context surfaces naturally.",
    "",
    "## Our Graph Structure",
    "",
    "```rust",
    "pub struct KnowledgeGraph {",
    "    nodes: HashMap<NodeId, Node>,",
    "    edges: HashMap<(NodeId, NodeId), Edge>,",
    "}",
    "",
    "pub struct Node {",
    "    id: NodeId,",
    "    content: String,",
    "    entity_type: EntityType,  // Person, Concept, Event, etc.",
    "    activation: f32,",
    "}",
    "",
    "pub struct Edge {",
    "    source: NodeId,",
    "    target: NodeId,",
    "    relation: RelationType,  // causes, contains, similar, etc.",
    "    strength: f32,",
    "}",
    "```",
    "",
    "## Entity Extraction",
    "",
    "When a memory is stored, we extract entities:",
    "",
    "```rust",
    "fn extract_entities(text: &str) -> Vec<Entity> {",
    "    let ner_results = ner_model.predict(text);",
    "    ner_results.iter().map(|r| Entity {",
    "        text: r.text.clone(),",
    "        entity_type: classify_type(&r.label),",
    "        confidence: r.score,",
    "    }).collect()",
    "}",
    "```",
    "",
    "Then we create/update graph nodes and edges between co-occurring entities.",
    "",
    "## Spreading Activation Algorithm",
    "",
    "```rust",
    "fn spread_activation(seed_nodes: &[NodeId], depth: usize) -> Vec<(NodeId, f32)> {",
    "    let mut activations: HashMap<NodeId, f32> = HashMap::new();",
    "",
    "    // Initialize seed nodes",
    "    for node in seed_nodes {",
    "        activations.insert(*node, 1.0);",
    "    }",
    "",
    "    // Spread for N iterations",
    "    for _ in 0..depth {",
    "        let mut new_activations = HashMap::new();",
    "        for (node, activation) in &activations {",
    "            for edge in self.edges_from(*node) {",
    "                let spread = activation * edge.strength * DECAY_FACTOR;",
    "                if spread > ACTIVATION_THRESHOLD {",
    "                    *new_activations.entry(edge.target).or_insert(0.0) += spread;",
    "                }",
    "            }",
    "        }",
    "        for (node, act) in new_activations {",
    "            *activations.entry(node).or_insert(0.0) += act;",
    "        }",
    "    }",
    "",
    "    activations.into_iter().sorted_by_key(|(_, a)| OrderedFloat(-a)).collect()",
    "}",
    "```",
    "",
    "## Proactive Context Retrieval",
    "",
    "When the user asks a question, we:",
    "",
    "1. Extract entities from the query",
    "2. Spread activation from those entities",
    "3. Retrieve memories connected to activated nodes",
    "4. Fuse with vector search results",
    "",
    "```rust",
    "fn proactive_context(query: &str) -> Vec<Memory> {",
    "    let query_entities = extract_entities(query);",
    "    let query_nodes = self.find_nodes(&query_entities);",
    "    let activated = spread_activation(&query_nodes, 3);",
    "",
    "    let graph_memories = activated.iter()",
    "        .flat_map(|(node, _)| self.memories_for_node(*node))",
    "        .collect();",
    "",
    "    let vector_memories = self.vectors.search(query, 10);",
    "",
    "    fuse_and_rank(graph_memories, vector_memories)",
    "}",
    "```",
    "",
    "## Example",
    "",
    "Query: \"How should I handle database errors?\"",
    "",
    "Entities extracted: [database, errors]",
    "",
    "Spreading activation from \"database\":",
    "- PostgreSQL (strength 0.9)",
    "- SQL (strength 0.7)",
    "- connection pool (strength 0.6)",
    "",
    "Spreading from \"errors\":",
    "- exception handling (strength 0.8)",
    "- logging (strength 0.5)",
    "- retry logic (strength 0.4)",
    "",
    "Combined context surfaces: PostgreSQL error handling best practices, the project's existing retry logic, and relevant logging patterns.",
  ],
  "benchmarking-memory-systems": [
    "# Benchmarking AI Memory Systems",
    "",
    "How do you measure whether an AI memory system is good? We share our methodology.",
    "",
    "## Key Metrics",
    "",
    "### 1. Retrieval Latency",
    "",
    "How fast can you get relevant context?",
    "",
    "| Operation | Target | shodh-memory |",
    "|-----------|--------|--------------|",
    "| Graph lookup | <10μs | 0.8μs |",
    "| Vector search (10 results) | <100ms | 42ms |",
    "| Proactive context | <50ms | 31ms |",
    "| Remember (store) | <100ms | 58ms |",
    "",
    "### 2. Retrieval Quality",
    "",
    "Do you get the RIGHT memories?",
    "",
    "We use two metrics:",
    "- **Precision@K**: Of top K results, how many are relevant?",
    "- **MRR (Mean Reciprocal Rank)**: Where does the first relevant result appear?",
    "",
    "On our coding-assistant benchmark:",
    "",
    "| System | Precision@5 | MRR |",
    "|--------|-------------|-----|",
    "| Vector-only baseline | 0.67 | 0.71 |",
    "| shodh-memory | 0.86 | 0.89 |",
    "",
    "### 3. Memory Scaling",
    "",
    "Does quality degrade with more memories?",
    "",
    "| Memory count | Latency (p50) | Quality (P@5) |",
    "|--------------|---------------|---------------|",
    "| 1,000 | 28ms | 0.89 |",
    "| 10,000 | 34ms | 0.87 |",
    "| 100,000 | 52ms | 0.84 |",
    "| 1,000,000 | 89ms | 0.81 |",
    "",
    "Sub-linear scaling. Quality degrades gracefully.",
    "",
    "## Benchmark Methodology",
    "",
    "### Dataset",
    "",
    "We created a coding-assistant memory dataset:",
    "- 10,000 memories from real coding sessions",
    "- 500 query-relevance pairs (human labeled)",
    "- Mix of code, decisions, preferences, errors",
    "",
    "### Test Protocol",
    "",
    "```python",
    "for query, relevant_memories in test_set:",
    "    start = time.perf_counter()",
    "    results = memory.recall(query, limit=10)",
    "    latency = time.perf_counter() - start",
    "",
    "    retrieved_ids = [r.id for r in results]",
    "    precision = len(set(retrieved_ids) & set(relevant_memories)) / len(retrieved_ids)",
    "    mrr = compute_mrr(retrieved_ids, relevant_memories)",
    "",
    "    record(latency, precision, mrr)",
    "```",
    "",
    "### Hardware",
    "",
    "All benchmarks run on:",
    "- Intel i7-12700K",
    "- 32GB RAM",
    "- NVMe SSD",
    "",
    "And also on Raspberry Pi 4 (4GB) for edge comparison.",
    "",
    "## Comparison to Alternatives",
    "",
    "We compared against:",
    "- Pinecone (cloud vector DB)",
    "- Chroma (local vector DB)",
    "- Plain PostgreSQL with pgvector",
    "",
    "| System | Latency | Quality | Offline? | Cost |",
    "|--------|---------|---------|----------|------|",
    "| Pinecone | 45ms | 0.69 | No | $$$ |",
    "| Chroma | 38ms | 0.67 | Yes | Free |",
    "| pgvector | 52ms | 0.65 | Yes | Free |",
    "| shodh-memory | 31ms | 0.86 | Yes | Free |",
    "",
    "The quality difference comes from our hybrid architecture (vectors + graph + temporal).",
    "",
    "## Reproducibility",
    "",
    "Our benchmarks are open source:",
    "",
    "```bash",
    "git clone https://github.com/varun29ankuS/shodh-memory",
    "cd shodh-memory",
    "cargo bench --bench memory_benchmarks",
    "```",
  ],
  "robotics-memory-real-world": [
    "# Memory for Robots: Learning from the Real World",
    "",
    "Theory is nice. Here's how shodh-memory works in an actual warehouse robot.",
    "",
    "## The Setup",
    "",
    "Our test subject: a pick-and-place robot in a small fulfillment center:",
    "- 6-axis arm",
    "- Mobile base",
    "- RGB-D camera",
    "- Running on NVIDIA Jetson Orin",
    "",
    "## The Problem",
    "",
    "Warehouses change. Products move. New SKUs arrive. Lighting shifts. A robot trained once degrades over time.",
    "",
    "Traditional approach: retrain periodically. Expensive. Downtime.",
    "",
    "Our approach: continuous learning with shodh-memory.",
    "",
    "## What the Robot Remembers",
    "",
    "### Object Locations",
    "",
    "```rust",
    "// After each successful pick",
    "memory.remember(",
    "    format!(\"SKU {} found at bin {} position {:?}\", sku, bin, pos),",
    "    tags: [\"location\", sku, bin]",
    ");",
    "```",
    "",
    "Over time, the robot builds a probabilistic map of where items are likely to be.",
    "",
    "### Grasp Strategies",
    "",
    "```rust",
    "// After each grasp attempt",
    "if success {",
    "    memory.remember(",
    "        format!(\"Grasp {} worked for {} at angle {}\", strategy, sku, angle),",
    "        tags: [\"grasp\", \"success\", sku]",
    "    );",
    "} else {",
    "    memory.remember(",
    "        format!(\"Grasp {} failed for {}: {}\", strategy, sku, reason),",
    "        tags: [\"grasp\", \"failure\", sku]",
    "    );",
    "}",
    "```",
    "",
    "The robot learns which grasp strategies work for which product types.",
    "",
    "### Environmental Conditions",
    "",
    "```rust",
    "// Periodic observations",
    "memory.remember(",
    "    format!(\"Lighting in zone A: {} lux, shadows from {:?}\", lux, shadow_dir),",
    "    tags: [\"environment\", \"lighting\", \"zone-a\"]",
    ");",
    "```",
    "",
    "Lighting affects vision. The robot remembers when shadows are problematic.",
    "",
    "## Retrieval in Action",
    "",
    "When given a pick task:",
    "",
    "```rust",
    "fn plan_pick(sku: &str) -> PickPlan {",
    "    // Where is this item likely to be?",
    "    let locations = memory.recall(",
    "        format!(\"Where is {} located?\", sku),",
    "        limit: 5",
    "    );",
    "",
    "    // What grasp strategy works?",
    "    let grasps = memory.recall(",
    "        format!(\"Successful grasp for {}\", sku),",
    "        limit: 3",
    "    );",
    "",
    "    // Current environmental conditions",
    "    let env = memory.proactive_context(",
    "        \"current lighting and obstacles\"",
    "    );",
    "",
    "    combine_into_plan(locations, grasps, env)",
    "}",
    "```",
    "",
    "## Results",
    "",
    "After 2 weeks of operation:",
    "",
    "| Metric | Day 1 | Day 14 |",
    "|--------|-------|--------|",
    "| Pick success rate | 87% | 94% |",
    "| Avg pick time | 4.2s | 3.1s |",
    "| Failed grasp retries | 23% | 8% |",
    "| Path planning time | 120ms | 85ms |",
    "",
    "The robot got better at its job without retraining.",
    "",
    "## Memory Growth",
    "",
    "After 2 weeks:",
    "- 47,000 memories stored",
    "- 180,000 graph edges",
    "- 12GB storage used",
    "",
    "Decay keeps memory bounded. Old, unused memories fade. Current knowledge stays fresh.",
    "",
    "## Lessons Learned",
    "",
    "1. **Tag everything**: Structured tags enable fast filtering",
    "2. **Remember failures**: Negative examples are as valuable as positive",
    "3. **Temporal context matters**: Recent observations outweigh old ones",
    "4. **Edge deployment works**: Sub-100ms latency on Jetson is achievable",
  ],
  "privacy-first-ai-memory": [
    "# Privacy-First AI Memory",
    "",
    "Your AI assistant knows a lot about you. Where does that knowledge live?",
    "",
    "## The Problem with Cloud Memory",
    "",
    "Most AI memory solutions send your data to servers you don't control:",
    "",
    "- Your code snippets",
    "- Your business decisions",
    "- Your personal preferences",
    "- Your proprietary algorithms",
    "",
    "This data trains models. Improves products. Generates revenue. For someone else.",
    "",
    "## What We Believe",
    "",
    "**Your agent's knowledge is YOUR intellectual property.**",
    "",
    "When an AI learns your coding patterns, that's valuable. When it learns your business logic, that's trade secret. When it learns your preferences, that's personal data.",
    "",
    "None of this should leave your hardware without your explicit consent.",
    "",
    "## How shodh-memory Stays Local",
    "",
    "### No Network Required",
    "",
    "```rust",
    "// The entire system runs locally",
    "let memory = Memory::new(\"./my-private-memory\")?;",
    "// No API keys, no cloud endpoints, no telemetry",
    "```",
    "",
    "The binary has zero network dependencies. Air-gapped systems work fine.",
    "",
    "### No Data Exfiltration",
    "",
    "We don't collect:",
    "- Usage statistics",
    "- Memory contents",
    "- Query logs",
    "- Crash reports",
    "",
    "We CAN'T collect this data. The code doesn't include collection mechanisms.",
    "",
    "### Auditable",
    "",
    "```bash",
    "# See exactly what's stored",
    "shodh-memory export --format json > my_memories.json",
    "",
    "# Delete everything",
    "rm -rf ~/.shodh-memory/data",
    "```",
    "",
    "You can inspect, export, and delete your memory at any time.",
    "",
    "## But What About Cloud AI?",
    "",
    "Claude, GPT-4, etc. run in the cloud. How does local memory help?",
    "",
    "The MCP protocol separates concerns:",
    "",
    "```",
    "You ←→ Cloud AI ←→ Local Memory",
    "",
    "1. You send query to cloud AI",
    "2. Cloud AI requests context from LOCAL memory server",
    "3. Memory server returns relevant memories",
    "4. Cloud AI responds with context",
    "5. Important: Cloud AI never sees ALL your memories",
    "                Only what's retrieved for this query",
    "```",
    "",
    "The cloud AI sees query-relevant context, not your entire memory database.",
    "",
    "## GDPR, HIPAA, SOC2",
    "",
    "Local-first memory simplifies compliance:",
    "",
    "- **GDPR Right to Erasure**: Delete the local database",
    "- **HIPAA Data Residency**: Data never leaves the facility",
    "- **SOC2 Access Control**: Standard filesystem permissions",
    "",
    "No third-party DPAs. No cloud provider audits. No cross-border transfer concerns.",
    "",
    "## The Trade-off",
    "",
    "Local memory means:",
    "- ✓ Privacy preserved",
    "- ✓ No vendor lock-in",
    "- ✓ Works offline",
    "- ✗ You manage backups",
    "- ✗ No cross-device sync (by default)",
    "",
    "For most use cases, this trade-off favors local.",
    "",
    "## Our Commitment",
    "",
    "Shodh-memory will always be:",
    "- Open source (MIT license)",
    "- Local-first",
    "- Telemetry-free",
    "",
    "Your memory. Your hardware. Your data.",
  ],
  "long-term-potentiation-code": [
    "# Long-Term Potentiation in Code",
    "",
    "In the brain, some memories become permanent. Not everything, but the important stuff. We implement the same principle.",
    "",
    "## What is LTP?",
    "",
    "Long-Term Potentiation (LTP) is a biological process where repeated synaptic activation causes lasting strengthening. First observed in rabbit hippocampi by Bliss and Lømo in 1973.",
    "",
    "Key properties:",
    "1. **Input specificity**: Only activated synapses strengthen",
    "2. **Cooperativity**: Multiple inputs strengthen more than one",
    "3. **Persistence**: Changes last hours to lifetime",
    "",
    "## The Problem with Uniform Decay",
    "",
    "Simple memory systems apply uniform decay:",
    "",
    "```rust",
    "// Naive approach",
    "for memory in memories {",
    "    memory.strength *= DECAY_RATE;  // Everything fades equally",
    "}",
    "```",
    "",
    "This loses important knowledge. The user's core preferences shouldn't decay just because they weren't mentioned today.",
    "",
    "## Our LTP Implementation",
    "",
    "### Strength Tracking",
    "",
    "Every memory and graph edge tracks:",
    "",
    "```rust",
    "pub struct MemoryMetadata {",
    "    strength: f32,           // Current strength (0.0 - 1.0)",
    "    access_count: u32,       // Total accesses",
    "    access_pattern: Vec<Timestamp>,  // Recent access times",
    "    permanent: bool,         // LTP achieved?",
    "}",
    "```",
    "",
    "### LTP Criteria",
    "",
    "A memory becomes permanent when:",
    "",
    "```rust",
    "fn check_ltp(memory: &Memory) -> bool {",
    "    // Minimum strength threshold",
    "    if memory.strength < 0.8 {",
    "        return false;",
    "    }",
    "",
    "    // Minimum access count",
    "    if memory.access_count < 10 {",
    "        return false;",
    "    }",
    "",
    "    // Spaced repetition pattern",
    "    let intervals = compute_intervals(&memory.access_pattern);",
    "    if !has_spaced_pattern(&intervals) {",
    "        return false;",
    "    }",
    "",
    "    true  // Achieves LTP",
    "}",
    "```",
    "",
    "The spaced pattern check ensures the memory was accessed over an extended period, not just burst accessed.",
    "",
    "### Immunity to Decay",
    "",
    "Once permanent, the memory is protected:",
    "",
    "```rust",
    "fn apply_decay(memory: &mut Memory) {",
    "    if memory.permanent {",
    "        // LTP memories don't decay",
    "        // But they can still be forgotten via explicit forget()",
    "        return;",
    "    }",
    "",
    "    // Normal decay for non-permanent memories",
    "    memory.strength *= calculate_decay_factor(memory);",
    "}",
    "```",
    "",
    "## What Becomes Permanent?",
    "",
    "In practice, LTP captures:",
    "",
    "- Core user preferences (\"prefers Rust\")",
    "- Frequently-used patterns (\"uses dependency injection\")",
    "- Key decisions (\"chose PostgreSQL for production\")",
    "",
    "Ephemeral context naturally fades:",
    "- Debugging sessions",
    "- One-off questions",
    "- Temporary workarounds",
    "",
    "## The Consolidation Report",
    "",
    "Track what's becoming permanent:",
    "",
    "```bash",
    "$ shodh-memory consolidation-report",
    "",
    "LTP Achieved (last 7 days):",
    "  - \"User prefers functional programming patterns\" (strength: 0.94)",
    "  - \"Project uses Next.js with App Router\" (strength: 0.91)",
    "  - \"Testing strategy: unit tests with Jest\" (strength: 0.88)",
    "",
    "Approaching LTP:",
    "  - \"Prefers explicit error handling over exceptions\" (7/10 accesses)",
    "  - \"Uses Tailwind CSS for styling\" (8/10 accesses)",
    "```",
    "",
    "## Results",
    "",
    "With LTP enabled:",
    "- Core knowledge retention: 100% (never decays)",
    "- Memory database size: -34% (ephemera naturally prunes)",
    "- Retrieval quality: +12% (less noise from outdated context)",
    "",
    "Memory should work like memory. Important things should last.",
  ],
};

export async function generateStaticParams() {
  return BLOG_POSTS.map((post) => ({
    slug: post.slug,
  }));
}

export default async function BlogPost({ params }: { params: Promise<{ slug: string }> }) {
  const { slug } = await params;
  const post = BLOG_POSTS.find((p) => p.slug === slug);

  if (!post) {
    notFound();
  }

  const content = BLOG_CONTENT[slug] || ["# Coming Soon", "", "This post is being written."];

  return (
    <div className="min-h-screen">
      <Header />
      <main className="pt-24 pb-16 px-4">
        <div className="mx-auto max-w-3xl">
          <Link href="/blog" className="text-[var(--term-text-dim)] hover:text-[var(--term-orange)] text-sm mb-8 inline-block">
            ← Back to blog
          </Link>

          <article>
            <header className="mb-8">
              <div className="flex items-center gap-4 text-sm text-[var(--term-text-dim)] mb-4">
                <span>{post.date}</span>
                <span>•</span>
                <span>{post.readTime} read</span>
              </div>
              <h1 className="text-2xl md:text-3xl font-semibold text-[var(--term-text)] mb-4">{post.title}</h1>
              <div className="flex gap-2">
                {post.tags.map((tag, i) => (
                  <span key={i} className="text-xs px-2 py-1 border border-[var(--term-border)] text-[var(--term-text-dim)]">
                    {tag}
                  </span>
                ))}
              </div>
            </header>

            <div className="shadow-window">
              <div className="terminal-header">
                <div className="terminal-dot terminal-dot-red" />
                <div className="terminal-dot terminal-dot-yellow" />
                <div className="terminal-dot terminal-dot-green" />
                <span className="ml-2 text-[var(--term-text-dim)] text-sm">{slug}.md</span>
              </div>
              <div className="terminal-body prose-terminal">
                {content.map((line, i) => (
                  <div key={i} className="leading-relaxed">
                    {line.startsWith("# ") ? (
                      <h1 className="text-xl font-bold text-[var(--term-orange)] mt-6 mb-4">{line.slice(2)}</h1>
                    ) : line.startsWith("## ") ? (
                      <h2 className="text-lg font-semibold text-[var(--term-orange)] mt-6 mb-3">{line.slice(3)}</h2>
                    ) : line.startsWith("### ") ? (
                      <h3 className="text-base font-medium text-[var(--term-text)] mt-4 mb-2">{line.slice(4)}</h3>
                    ) : line.startsWith("```") ? (
                      <div className="text-[var(--term-text-dim)] text-xs">{line}</div>
                    ) : line.startsWith("- ") || line.startsWith("* ") ? (
                      <div className="text-[var(--term-text-dim)] pl-4">• {line.slice(2)}</div>
                    ) : line.startsWith("|") ? (
                      <div className="text-[var(--term-text-dim)] font-mono text-xs">{line}</div>
                    ) : line === "" ? (
                      <div className="h-4" />
                    ) : (
                      <p className="text-[var(--term-text-dim)]">{line}</p>
                    )}
                  </div>
                ))}
              </div>
            </div>
          </article>

          <div className="mt-12 pt-8 border-t border-[var(--term-border)]">
            <Link href="/blog" className="text-[var(--term-orange)] hover:underline">
              ← More posts
            </Link>
          </div>
        </div>
      </main>
      <Footer />
    </div>
  );
}
